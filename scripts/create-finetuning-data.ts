import * as fs from "fs/promises"
import * as path from "path"
import { EOL } from "os"

/**
 * @file This script processes conversation logs generated by ConversationLogger.ts
 * and transforms them into a supervised fine-tuning dataset format compatible with
 * Google's Gemini models on Vertex AI.
 *
 * It reads .jsonl log files from a specified input directory (default: .roo-logs),
 * processes each session log into one or more Gemini-compatible examples, and
 * writes the output to a new directory (default: finetuning-datasets), with one
 * file per session.
 *
 * The goal is to leverage high-quality conversation data from powerful models
 * like Claude to train other models for specific agentic AI tasks within Roo Code.
 *
 * @see https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini-supervised-tuning-prepare
 * @see https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-function-calling
 */

// Interfaces based on ConversationLogger.ts and Gemini's format
interface LogEntry {
	timestamp: string
	session_id: string
	type: "user_message" | "ai_response" | "tool_call"
	mode: string
	content?: string
	tool_calls?: { name: string; input: any }[]
	tool_name?: string
	parameters?: any
	result?: any
}

interface GeminiMessage {
	role: "user" | "model" | "tool"
	parts: ({ text: string } | { tool_code: any } | { tool_result: any })[]
}

interface GeminiExample {
	messages: GeminiMessage[]
}

/**
 * Parses command-line arguments.
 * @param args - Command-line arguments array.
 * @returns Parsed arguments with input and output paths.
 */
function parseArguments(args: string[]): { input: string; output: string } {
	const inputFlag = "--input"
	const outputFlag = "--output"
	let input = ".roo-logs" // default input directory
	let output = "finetuning-datasets" // default output directory

	for (let i = 0; i < args.length; i++) {
		const arg = args[i]
		if (arg === inputFlag) {
			const value = args[i + 1]
			if (value) {
				input = value
			}
		} else if (arg === outputFlag) {
			const value = args[i + 1]
			if (value) {
				output = value
			}
		}
	}

	return { input, output }
}

/**
 * Finds all .jsonl files in a directory.
 * @param dir - The directory to search.
 * @returns A promise that resolves to an array of file paths.
 */
async function findLogFiles(dir: string): Promise<string[]> {
	try {
		const dirents = await fs.readdir(dir, { withFileTypes: true })
		const files = await Promise.all(
			dirents.map(async (dirent) => {
				const res = path.resolve(dir, dirent.name)
				if (dirent.isDirectory()) {
					return findLogFiles(res)
				}
				return res.endsWith(".jsonl") ? res : []
			}),
		)
		return Array.prototype.concat(...files)
	} catch (error) {
		console.error(`Error reading directory ${dir}:`, error)
		return []
	}
}

/**
 * Processes a single session log file and converts it to fine-tuning format.
 * @param filePath - The path to the log file.
 * @returns A promise that resolves to an array of GeminiExample objects.
 */
async function processLogFile(filePath: string): Promise<GeminiExample[]> {
	const fileContent = await fs.readFile(filePath, "utf-8")
	const lines = fileContent.split(/\r?\n/).filter((line) => line)
	const logEntries: LogEntry[] = lines
		.map((line) => {
			try {
				return JSON.parse(line)
			} catch (error) {
				console.warn(`Skipping malformed log entry: ${line}`)
				return null
			}
		})
		.filter((entry): entry is LogEntry => entry !== null)

	const examples: GeminiExample[] = []
	let i = 0

	while (i < logEntries.length) {
		const currentEntry = logEntries[i]
		if (currentEntry?.type === "user_message") {
			const turn: LogEntry[] = []
			let j = i
			// Collect all entries until the next user message
			while (j < logEntries.length && (j === i || logEntries[j]?.type !== "user_message")) {
				const entry = logEntries[j]
				if (entry) {
					turn.push(entry)
				}
				j++
			}

			// Process the collected turn
			const messages: GeminiMessage[] = []
			const firstMessage = turn[0]
			if (!firstMessage) {
				i = j
				continue
			}

			messages.push({ role: "user", parts: [{ text: firstMessage.content ?? "" }] })

			const modelResponses: GeminiMessage[] = []
			const toolResults: GeminiMessage[] = []

			for (let k = 1; k < turn.length; k++) {
				const entry = turn[k]
				if (!entry) continue

				if (entry.type === "ai_response") {
					const modelResponseParts: ({ text: string } | { tool_code: any })[] = []
					if (entry.content) {
						modelResponseParts.push({ text: entry.content })
					}
					if (entry.tool_calls && entry.tool_calls.length > 0) {
						modelResponseParts.push(
							...entry.tool_calls.map((tc) => ({ tool_code: { name: tc.name, args: tc.input } })),
						)
					}
					if (modelResponseParts.length > 0) {
						modelResponses.push({ role: "model", parts: modelResponseParts })
					}
				} else if (entry.type === "tool_call") {
					const toolOutput =
						typeof entry.result === "string" ? entry.result : JSON.stringify(entry.result, null, 2)

					toolResults.push({
						role: "tool",
						parts: [
							{
								tool_result: {
									name: entry.tool_name!,
									response: toolOutput,
								},
							},
						],
					})
				}
			}

			// Re-order the messages to ensure the model's tool call comes before the tool result.
			const modelResponsesWithToolCalls = modelResponses.filter((m) => m.parts.some((p) => "tool_code" in p))
			const modelSummaries = modelResponses.filter((m) => !m.parts.some((p) => "tool_code" in p))

			if (modelResponsesWithToolCalls.length > 0 && toolResults.length > 0) {
				messages.push(...modelResponsesWithToolCalls)
				messages.push(...toolResults)
				messages.push(...modelSummaries)

				examples.push({ messages })
			}
			i = j
		} else {
			i++
		}
	}
	return examples
}

/**
 * Main function to run the script.
 */
async function main() {
	const { input, output } = parseArguments(process.argv.slice(2))
	const workspaceRoot = process.cwd()
	const inputDir = path.resolve(workspaceRoot, input)
	const outputDir = path.resolve(workspaceRoot, output)

	await fs.mkdir(outputDir, { recursive: true })

	console.log(`Starting conversion...`)
	console.log(`Input directory: ${inputDir}`)
	console.log(`Output directory: ${outputDir}`)

	const logFiles = await findLogFiles(inputDir)

	if (logFiles.length === 0) {
		console.log("No .jsonl log files found. Exiting.")
		return
	}

	let totalExamples = 0
	for (const file of logFiles) {
		const sessionId = path.basename(file, ".jsonl")
		const outputFileName = `finetuning-dataset-${sessionId}.jsonl`
		const outputFile = path.join(outputDir, outputFileName)

		const examples = await processLogFile(file)

		if (examples.length > 0) {
			const outputContent = examples.map((ex) => JSON.stringify(ex)).join(EOL)
			await fs.writeFile(outputFile, outputContent, "utf-8")
			console.log(
				`Successfully created fine-tuning dataset for session ${sessionId} with ${examples.length} examples at ${outputFile}`,
			)
			totalExamples += examples.length
		}
	}

	if (totalExamples > 0) {
		console.log(`\nFinished. Total examples generated: ${totalExamples}.`)
	} else {
		console.log("No valid training examples could be generated from the logs.")
	}
}

main().catch((error) => {
	console.error("An unexpected error occurred:", error)
	process.exit(1)
})
